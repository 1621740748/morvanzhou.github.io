---
layout: tutorial-post
title: 加速神经网络训练 Speed Up Training
description: "今天我们会来聊聊怎么样加速你的神经网络训练过程. 里面的方法包括: 
Stochastic Gradient Descent (SGD);
Momentum;
AdaGrad;
RMSProp;
Adam."
source: https://www.youtube.com/watch?v=UlUGGB7akfE&index=12&list=PLXO45tsB95cIFm8Y8vMkNNPPXAtYXwKin
category: ML-intro
comments: true
tags: 机器学习, 简介
date: 2016-11-3
published: true
chapter: 2
---

<iframe width="560" height="315" src="https://www.youtube.com/embed/UlUGGB7akfE?list=PLXO45tsB95cIFm8Y8vMkNNPPXAtYXwKin" frameborder="0" allowfullscreen></iframe>
<p class="link-under-youtube">(观看优酷的视频请<a href="http://v.youku.com/v_show/id_XMTc2MjA0ODQyOA==.html?f=27892935&o=1" target="_blank">点击这里)</a></p>

## {{ page.title }}
今天我们会来聊聊怎么样加速你的神经网络训练过程.
里面的方法包括: 
Stochastic Gradient Descent (SGD);
Momentum;
AdaGrad;
RMSProp;
Adam.

英文学习资料: http://sebastianruder.com/optimizing-gradient-descent/
