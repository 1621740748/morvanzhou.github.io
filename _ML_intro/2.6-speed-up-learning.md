---
layout: tutorial-post
title: 加速神经网络训练 Speed Up Training
description: "今天我们会来聊聊怎么样加速你的神经网络训练过程. 里面的方法包括: 
Stochastic Gradient Descent (SGD);
Momentum;
AdaGrad;
RMSProp;
Adam."
youtube_id: #
youku_link: #
category: ML-intro
comments: true
tags: 机器学习, 简介
date: 2016-11-3
published: true
chapter: 2
---

今天我们会来聊聊怎么样加速你的神经网络训练过程.
里面的方法包括: 
Stochastic Gradient Descent (SGD);
Momentum;
AdaGrad;
RMSProp;
Adam.

学习资料: 
  * 英文学习[资料](http://sebastianruder.com/optimizing-gradient-descent/)
